import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.utils import to_categorical

# Sample text data - in a real application, you would use a large dataset
text_data = """This is a sample text for training the autocorrect keyboard system. 
It predicts the next word based on context. The goal is to create an intuitive keyboard that improves user experience by accurately anticipating the next word. 
This system leverages contextual information to enhance predictive capabilities."""

# Preprocessing the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text_data])
total_words = len(tokenizer.word_index) + 1

# Create input sequences using n-grams approach
input_sequences = []

for line in text_data.split('.'):
    line = line.strip()
    if not line:
        continue
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i+1]
        input_sequences.append(n_gram_sequence)

# Pad sequences to ensure equal length
max_seq_len = max(len(seq) for seq in input_sequences)
input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')

# Create predictors and label
X = input_sequences[:, :-1]  # all words except last
labels = input_sequences[:, -1]  # last word to predict

# One-hot encode the labels
y = to_categorical(labels, num_classes=total_words)

# Build the model
model = Sequential()
model.add(Embedding(total_words, 100, input_length=max_seq_len - 1))
model.add(LSTM(150))
model.add(Dense(total_words, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X, y, epochs=100, verbose=1)

# Function to predict the next word given a sequence of words
def predict_next_word(model, tokenizer, text, max_seq_len):
    text = text.lower()
    token_list = tokenizer.texts_to_sequences([text])[0]
    token_list = pad_sequences([token_list], maxlen=max_seq_len -1, padding='pre')
    predicted_probs = model.predict(token_list, verbose=0)
    predicted_index = np.argmax(predicted_probs, axis=-1)[0]
    for word, index in tokenizer.word_index.items():
        if index == predicted_index:
            return word
    return ""

# Example usage
if __name__ == "__main__":
    input_text = "this system leverages"
    next_word = predict_next_word(model, tokenizer, input_text, max_seq_len)
    print(f"Input text: '{input_text}'")
    print(f"Predicted next word: '{next_word}'")

